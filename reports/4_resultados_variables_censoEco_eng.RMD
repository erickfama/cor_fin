---
title: "Activities Report No. 4"
subtitle: "Models trained with Economic Census data"
author:
- "Research assistant: Erick Gabriel Fajardo Martínez"
- "Researcher: Dr. Gabriel Purón Cid"
date: '`r format(Sys.time(), "%d %B %Y")`'
fontsize: 12pt
output:   
  pdf_document:
    latex_engine: xelatex
header-includes:
 \usepackage{float}
---

```{r setup, include=FALSE}
Sys.setlocale("LC_TIME", "English")
set.seed(123)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.pos="H")
```

```{r librerias, echo=FALSE}
# Librerias ----
library(tidyverse)
# library(ggtext)
library(here)
library(kableExtra)
```

```{r lectura, echo=FALSE}
## Egresos ----
eg_per_predicted <- read_csv(here("./data/3_final/eg_per_predicted.csv")) %>%
  rename(corrup_hat_eg_per = corrup_hat, corrup_eg_per = corrup)
eg_inc_predicted <- read_csv(here("./data/3_final/eg_inc_predicted.csv")) %>%
  rename(corrup_hat_eg_inc = corrup_hat, corrup_eg_inc = corrup)

## Ingresos ----
ig_per_predicted <- read_csv(here("./data/3_final/ig_per_predicted.csv")) %>%
  rename(corrup_hat_ig_per = corrup_hat, corrup_ig_per = corrup)
ig_inc_predicted <- read_csv(here("./data/3_final/ig_inc_predicted.csv")) %>%
  rename(corrup_hat_ig_inc = corrup_hat, corrup_ig_inc = corrup)

## Modelos ----
model_eg_per <- readRDS(here("./models/model_eg_per.rds"))
model_eg_inc <- readRDS(here("./models/model_eg_inc.rds"))
model_ig_per <- readRDS(here("./models/model_ig_per.rds"))
model_ig_inc <- readRDS(here("./models/model_ig_inc.rds"))

## Mapa ----
mun_mapa <- sf::read_sf(here("./data/maps/muni.shp"))

# cutoff 
best_cutoff <- function(tipo_fin, method){
  if(tipo_fin == "eg"){
    cutoff <- read_csv("./data/2_interim/eval_eg.csv", locale = locale(encoding = "UTF-8"), show_col_types = FALSE) %>%
      select(method, best_cutoff) 
    if(method == "per"){
      value <- cutoff %>% filter(method == "Percepción") %>% select(best_cutoff) %>% pull() %>% unique() %>% ifelse(is.null(.), 0, .)
    } else {
      value <- cutoff %>% filter(method == "Incidencia") %>% select(best_cutoff) %>% pull() %>% unique() %>% ifelse(is.null(.), 0, .)
    }
  } 
  if(tipo_fin == "ig"){
    cutoff <- read_csv("./data/2_interim/eval_ig.csv", locale = locale(encoding = "latin1"), show_col_types = FALSE) %>%
      select(method, best_cutoff) 
    if(method == "per"){
      value <- cutoff %>% filter(method == "Percepción") %>% select(best_cutoff) %>% pull() %>% unique() %>% ifelse(is.null(.), 0, .)
    } else {
      value <- cutoff %>% filter(method == "Incidencia") %>% select(best_cutoff) %>% pull() %>% unique() %>% ifelse(is.null(.), 0, .)
    }
  }
  value <- ifelse(is.null(value), 0, value)
  return(value)
}
```

# Summary

In previous reports Mexican municipalities were classified into corrupt or not corrupt based on ENCIG's 2019 results about perception and incidence of corruption within the municipalitie and public finance data of each municipalitie. Therefore two classifications were made for all municipalities that were included in ENCIG's 2019 sample, one corresponding to the perception component and another to incidence. For the case of perception, municipalities were classified into corrupt or not corrupt by calculating population proportion that reported a high perception of corruption in their municipalitie of residence and if the proportion exceeded a certain threshold the municipalitie was labeled as corrupt and not corrupt otherwise. For the case of incidence, the classification followed the same method but the only difference was in the population proportion which was calculated as the proportion that reported been involved in an act of corruption. The classification has been perfomed with the implementation of *Gradient Boosting* a machine learning algorithm which is a great classification tool for the creation of very accurate models (Burkov, 2019).

Four models were trained of which can be divided into two big categories: 1) Public revenue and 2) Public expenditure; and each of this categories contains two models that correspond to the classification of corruption according to perception and incidence data.The purpose of this is to check if there is any variation across models that can be explained by the use of perception or incidence data about corruption. Models classification is resumed the following way:

\begin{minipage}[t]{0.5\textwidth}
\textbf{Public expenditure models}\\
\begin{itemize}
    \item Classification of corruption according to incidence data\\
    \item Classification of corruption according to perception data\\
\end{itemize}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
\textbf{Public revenue models}\\
\begin{itemize}
    \item Classification of corruption according to incidence data\\
    \item Classification of corruption according to perception data\\
\end{itemize}
\end{minipage}

# Economic Census Data

This report adds up data from the 2019 Economic Census which give a better performance to all models. The variables that were chosen are the following:

```{r variables_censo, echo=FALSE}
variables_censo <- read_csv(here("./reports/variables_censo.csv"), skip = 1)
kable(variables_censo, 
                 align = "c", 
                 format = "latex",
                 caption = "2019 Economic Census variables") %>%
  kableExtra::kable_styling(position = "center", latex_options = c("scale_down", "HOLD_position"))
```

All the variables are repeated by each economic sector according to 2018 North America's Industrial Classification System, which is divided into 20 main economic sectors. 187 is the total number of variables that were added from the economic census.
The complete data sets contain more than 500 variables for the case of public revenue, and more than 300 variables for the case of public expenditure.

# Model performance

The following table shows evaluation metrics for each trained model. Overall, model performance is improved by including data from the economic census, and this improvement is better shown in the sensibility of the models which is their hability of correctly classify corrupt municipalities.

Another improvement were in F1-score values for public expenditure models, meaning that both types of models are more balanced in terms of sensitivity and specificity. On the other hand, public revenue models did not show better results which can be explained by two reasons: 1) Optimization method used to set the treshold for initial classification and 2) lower number of total variables in the public revenue dataset.

```{r, desempeno, echo=FALSE}
sum_model <- function(model_cm, name){
  cm <- model_cm
  data.frame(Modelo = c(name),
             Sensibilidad = c(round(c(cm$byClass[c("Sensitivity")]), 2)),
             Especificidad = c(round(c(cm$byClass[c("Specificity")]), 2)),
             F1_score = round(c(cm$byClass[c("F1")]), 2),
             Precisión_balanceada = round(c(cm$byClass[c("Balanced Accuracy")]), 2))
}

desemp <- rbind(sum_model(model_eg_per$cm, "Expenditure - Perception"), 
      sum_model(model_eg_inc$cm, "Expenditure - Incidence"),
      sum_model(model_ig_per$cm, "Revenue - Perception"),
      sum_model(model_ig_inc$cm, "Revenue - Incidence"),
      make.row.names = FALSE)

kable(desemp, 
      align = "c", 
      format = "latex",
      caption = "Performance") %>%
  kableExtra::kable_styling(position = "center", latex_options = c("scale_down", "HOLD_position"))
```

## Variable importance

The following tables show all the most important variables in terms of classification impact for each model.

```{r eg_per_vars, echo=FALSE}
kable(model_eg_per$var_imp$importance %>% slice_head(n = 10), 
      align = "c", 
      format = "latex",
      caption = "Variable importance. Expenditure - Perception") %>%
  kableExtra::kable_styling(position = "center", latex_options = c("scale_down", "HOLD_position"))
```


```{r eg_inc_vars, echo=FALSE}
kable(model_eg_inc$var_imp$importance %>% slice_head(n = 10), 
      align = "c", 
      format = "latex",
      caption = "Variable importance. Expenditure - Incidence") %>%
  kableExtra::kable_styling(position = "center", latex_options = c("scale_down", "HOLD_position"))
```


```{r ig_per_vars, echo=FALSE}
kable(model_ig_per$var_imp$importance %>% slice_head(n = 10), 
      align = "c", 
      format = "latex",
      caption = "Variable importance. Revenue - Perception") %>%
  kableExtra::kable_styling(position = "center", latex_options = c("scale_down", "HOLD_position"))
```


```{r ig_inc_vars, echo=FALSE}
kable(model_ig_inc$var_imp$importance %>% slice_head(n = 10), 
      align = "c", 
      format = "latex",
      caption = "Variable importance. Revenue - Incidence") %>%
  kableExtra::kable_styling(position = "center", latex_options = c("scale_down", "HOLD_position"))
```

# Reference

Burkov, A. (2019). *The hundred-page machine learning book.* Andriy Burkov